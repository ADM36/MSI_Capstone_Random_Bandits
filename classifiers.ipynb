{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files & create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_files = glob.glob(\"/home/bryan/Documents/Code/si699/data/mturk_results/*.csv\")\n",
    "\n",
    "# HITId is a unique identifier for a task (text fragment given to Turkers to label). The text used for labeling could have been duplicated across trials (e.g. gold samples used more than once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5 #threshold for setting a label to 1/Directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_df shape: (341, 4)\n"
     ]
    }
   ],
   "source": [
    "# iterate through all files & append to ground_truth_df\n",
    "\n",
    "errors = []\n",
    "error_messages = []\n",
    "ground_truth_df = pd.DataFrame()\n",
    "\n",
    "for file in these_files:\n",
    "    try:\n",
    "        this_df = pd.read_csv(file, index_col=0)\n",
    "\n",
    "        ground_truth = this_df.groupby(\"HITId\").mean()[\"Answer.yes.1\"]\n",
    "        ground_truth = ground_truth.to_frame()\n",
    "\n",
    "        new_values = []\n",
    "        for value in ground_truth.values:\n",
    "            if value < THRESHOLD:\n",
    "                new_values.append(0)\n",
    "            else:\n",
    "                new_values.append(1)\n",
    "\n",
    "        ground_truth[\"new_values\"] = new_values\n",
    "        ground_truth = ground_truth.merge(this_df[[\"HITId\",\"Input.TEXT\"]], how = \"left\", on = \"HITId\")\n",
    "        ground_truth = ground_truth.drop_duplicates()\n",
    "        \n",
    "        ground_truth_df = ground_truth_df.append(ground_truth)\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors.append(file)\n",
    "        error_messages.append(e)\n",
    "        \n",
    "print(f\"ground_truth_df shape: {ground_truth_df.shape}\")\n",
    "ground_truth_df.drop(columns=\"HITId\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors,error_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve duplicate gold samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples = ground_truth_df[~ground_truth_df.duplicated(\"Input.TEXT\", keep=False)].sort_values(\"Input.TEXT\")\n",
    "gold_samples = ground_truth_df[ground_truth_df.duplicated(\"Input.TEXT\", keep=False)].sort_values(\"Input.TEXT\")\n",
    "\n",
    "gold_samples = gold_samples.groupby(\"Input.TEXT\").mean()\n",
    "\n",
    "new_values = []\n",
    "for value in gold_samples[\"Answer.yes.1\"]:\n",
    "            if value < THRESHOLD:\n",
    "                new_values.append(0)\n",
    "            else:\n",
    "                new_values.append(1)\n",
    "gold_samples[\"new_values\"] = new_values\n",
    "\n",
    "gold_samples.reset_index(inplace = True)\n",
    "ground_truth_df = pd.concat([good_samples,gold_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD ADDL COMMON FEATURES HERE (WHERE DATA LEAKAGE ISN'T POSSIBLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df[\"indicator_question\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"\\?\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "\n",
    "# simply binary flags\n",
    "# ground_truth_df[\"indicator_will\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"[Ww]ill\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "# ground_truth_df[\"indicator_would\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"[Ww]ould\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "# ground_truth_df[\"indicator_could\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"[Cc]ould\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "# ground_truth_df[\"indicator_can\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"[Cc]an\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "# ground_truth_df[\"indicator_dont\"] = ground_truth_df.apply(lambda x: 1 if re.search(\"[Dd]on't\",x[\"Input.TEXT\"]) else 0, axis=1)\n",
    "\n",
    "# counts of occurrences\n",
    "ground_truth_df[\"indicator_will\"] = ground_truth_df.apply(lambda x: len(re.findall(r\"\\b[Ww]ill\\b\",x[\"Input.TEXT\"])), axis=1)\n",
    "ground_truth_df[\"indicator_would\"] = ground_truth_df.apply(lambda x: len(re.findall(r\"\\b[Ww]ould\\b\",x[\"Input.TEXT\"])), axis=1)\n",
    "ground_truth_df[\"indicator_could\"] = ground_truth_df.apply(lambda x: len(re.findall(r\"\\b[Cc]ould\\b\",x[\"Input.TEXT\"])), axis=1)\n",
    "ground_truth_df[\"indicator_can\"] = ground_truth_df.apply(lambda x: len(re.findall(r\"\\b[Cc]an\\b\",x[\"Input.TEXT\"])), axis=1)\n",
    "ground_truth_df[\"indicator_dont\"] = ground_truth_df.apply(lambda x: len(re.findall(r\"\\b[Dd]on't\\b\",x[\"Input.TEXT\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df[\"readability\"] = ground_truth_df.apply(lambda x: textstat.text_standard(x[\"Input.TEXT\"], float_output=True), axis=1) #estimated school grade level required to understand the text, lower is easier\n",
    "ground_truth_df[\"flesch_reading_ease\"] = ground_truth_df.apply(lambda x: textstat.flesch_reading_ease(x[\"Input.TEXT\"]), axis=1) #Flesch Reading Ease, lower is more confusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer.yes.1</th>\n",
       "      <th>new_values</th>\n",
       "      <th>indicator_question</th>\n",
       "      <th>indicator_will</th>\n",
       "      <th>indicator_would</th>\n",
       "      <th>indicator_could</th>\n",
       "      <th>indicator_can</th>\n",
       "      <th>indicator_dont</th>\n",
       "      <th>readability</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.296482</td>\n",
       "      <td>0.327465</td>\n",
       "      <td>0.566901</td>\n",
       "      <td>0.373239</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.038732</td>\n",
       "      <td>0.327465</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>4.573944</td>\n",
       "      <td>94.748697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.343927</td>\n",
       "      <td>0.470117</td>\n",
       "      <td>0.496379</td>\n",
       "      <td>0.595735</td>\n",
       "      <td>0.417957</td>\n",
       "      <td>0.193297</td>\n",
       "      <td>0.689512</td>\n",
       "      <td>0.642248</td>\n",
       "      <td>3.235977</td>\n",
       "      <td>12.833385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>42.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>89.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.236458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>95.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>104.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>117.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Answer.yes.1  new_values  indicator_question  indicator_will  \\\n",
       "count    284.000000  284.000000          284.000000      284.000000   \n",
       "mean       0.296482    0.327465            0.566901        0.373239   \n",
       "std        0.343927    0.470117            0.496379        0.595735   \n",
       "min        0.000000    0.000000            0.000000        0.000000   \n",
       "25%        0.000000    0.000000            0.000000        0.000000   \n",
       "50%        0.236458    0.000000            1.000000        0.000000   \n",
       "75%        0.500000    1.000000            1.000000        1.000000   \n",
       "max        1.000000    1.000000            1.000000        3.000000   \n",
       "\n",
       "       indicator_would  indicator_could  indicator_can  indicator_dont  \\\n",
       "count       284.000000       284.000000     284.000000      284.000000   \n",
       "mean          0.126761         0.038732       0.327465        0.309859   \n",
       "std           0.417957         0.193297       0.689512        0.642248   \n",
       "min           0.000000         0.000000       0.000000        0.000000   \n",
       "25%           0.000000         0.000000       0.000000        0.000000   \n",
       "50%           0.000000         0.000000       0.000000        0.000000   \n",
       "75%           0.000000         0.000000       0.000000        0.000000   \n",
       "max           3.000000         1.000000       5.000000        4.000000   \n",
       "\n",
       "       readability  flesch_reading_ease  \n",
       "count   284.000000           284.000000  \n",
       "mean      4.573944            94.748697  \n",
       "std       3.235977            12.833385  \n",
       "min      -3.000000            42.380000  \n",
       "25%       3.000000            89.290000  \n",
       "50%       4.000000            95.880000  \n",
       "75%       6.000000           104.340000  \n",
       "max      17.000000           117.360000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ground_truth_df[[\"Input.TEXT\"]], ground_truth_df[\"new_values\"],train_size = 0.7, random_state = 444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD ADDL FEATURES HERE (WHERE DATA LEAKAGE IS POSSIBLE)\n",
    "* e.g. standardization should be done on training data by itself & then applied to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "word_vectorizer = TfidfVectorizer(strip_accents = \"unicode\", lowercase = True, \n",
    "                             stop_words = \"english\", analyzer = \"word\", token_pattern = \"[a-z]+\", smooth_idf = True, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer([('tfidf', word_vectorizer, 'Input.TEXT')], remainder=\"passthrough\")\n",
    "\n",
    "train_transformed = column_trans.fit_transform(X_train)\n",
    "test_transformed = column_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy = \"stratified\", random_state = 8)\n",
    "# dummy = DummyClassifier(strategy = \"most_frequent\", random_state = 8)\n",
    "\n",
    "dummy.fit(train_transformed, y_train)\n",
    "dummy_preds = dummy.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 48\n",
      "FP: 17\n",
      "FN: 12\n",
      "TP: 9\n",
      "Precision: 0.35\n",
      "Recall: 0.43\n",
      "F1: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, dummy_preds).ravel()\n",
    "print(f\"TN: {tn}\\nFP: {fp}\\nFN: {fn}\\nTP: {tp}\")\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = round((2*precision*recall)/(precision+recall),3)\n",
    "\n",
    "print(f\"Precision: {round(precision,2)}\")\n",
    "print(f\"Recall: {round(recall,2)}\")\n",
    "print(f\"F1: {round(f1,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/Documents/Code/python-environments/SI699/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
      "/home/bryan/Documents/Code/python-environments/SI699/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 s, sys: 145 ms, total: 3.87 s\n",
      "Wall time: 11.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=8, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'C': [10000, 1000, 10, 1],\n",
       "                          'class_weight': [None, 'balanced'],\n",
       "                          'fit_intercept': [True, False], 'n_jobs': [-1],\n",
       "                          'penalty': ['l2', 'l1'], 'solver': ['liblinear']},\n",
       "                         {'C': [10000, 1000, 10, 1],\n",
       "                          'class_weight': [None, 'balanced'],\n",
       "                          'fit_intercept': [True, False], 'n_jobs': [-1],\n",
       "                          'penalty': ['l2'], 'solver': ['lbfgs', 'saga']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(random_state=8)\n",
    "\n",
    "# lr_param_grid = [{\"penalty\":[\"l2\",\"none\",\"l1\"],\n",
    "#               \"C\":[10000,1000],\n",
    "#                   \"fit_intercept\": [True, False],\n",
    "#                   \"class_weight\": [None, \"balanced\"],\n",
    "# #                   \"solver\": [\"newton-cg\",\"sag\",\"saga\"]\n",
    "#                   \"solver\": [\"lbfgs\",\"liblinear\", \"saga\"]\n",
    "#               }]\n",
    "\n",
    "lr_param_grid = [{\"penalty\":[\"l2\",\"l1\"],\n",
    "              \"C\":[10000,1000,10,1],\n",
    "                  \"fit_intercept\": [True, False],\n",
    "                  \"class_weight\": [None, \"balanced\"],\n",
    "                  \"solver\": [\"liblinear\"],\n",
    "                  \"n_jobs\": [-1]\n",
    "              },\n",
    "                 {\"penalty\":[\"l2\"],\n",
    "                  \"C\":[10000,1000, 10, 1],\n",
    "                  \"fit_intercept\": [True, False],\n",
    "                  \"class_weight\": [None, \"balanced\"],\n",
    "                  \"solver\": [\"lbfgs\", \"saga\"],\n",
    "                  \"n_jobs\": [-1] \n",
    "              }]\n",
    "lr_gs = GridSearchCV(\n",
    "    estimator = lr,\n",
    "    param_grid = lr_param_grid,\n",
    "    cv=10\n",
    ")\n",
    "lr_gs.fit(train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000,\n",
       " 'class_weight': 'balanced',\n",
       " 'fit_intercept': True,\n",
       " 'n_jobs': -1,\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                   random_state=8, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_gs.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 59\n",
      "FP: 6\n",
      "FN: 14\n",
      "TP: 7\n",
      "Precision: 0.54\n",
      "Recall: 0.33\n",
      "F1: 0.41\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_preds).ravel()\n",
    "print(f\"TN: {tn}\\nFP: {fp}\\nFN: {fn}\\nTP: {tp}\")\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = round((2*precision*recall)/(precision+recall),3)\n",
    "\n",
    "print(f\"Precision: {round(precision,2)}\")\n",
    "print(f\"Recall: {round(recall,2)}\")\n",
    "print(f\"F1: {round(f1,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=8, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'C': [10000, 1000, 10, 1],\n",
       "                          'class_weight': ['balanced', None], 'degree': [2, 3],\n",
       "                          'gamma': ['scale', 'auto'],\n",
       "                          'kernel': ['linear', 'rbf', 'poly']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_param_grid = [{\"C\":[10000,1000,10,1],\n",
    "                   \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                   \"degree\": [2,3],\n",
    "                   \"gamma\": [\"scale\",\"auto\"],\n",
    "                  \"class_weight\": [\"balanced\",None]\n",
    "                  }]\n",
    "\n",
    "svm_gs = GridSearchCV(\n",
    "    estimator = svm,\n",
    "    param_grid = svm_param_grid,\n",
    "    cv=10\n",
    ")\n",
    "svm_gs.fit(train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'class_weight': 'balanced',\n",
       " 'degree': 2,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'linear'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=2, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=8, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = svm_gs.predict(test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 57\n",
      "FP: 8\n",
      "FN: 14\n",
      "TP: 7\n",
      "Precision: 0.47\n",
      "Recall: 0.33\n",
      "F1: 0.39\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, svm_preds).ravel()\n",
    "print(f\"TN: {tn}\\nFP: {fp}\\nFN: {fn}\\nTP: {tp}\")\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = round((2*precision*recall)/(precision+recall),3)\n",
    "\n",
    "print(f\"Precision: {round(precision,2)}\")\n",
    "print(f\"Recall: {round(recall,2)}\")\n",
    "print(f\"F1: {round(f1,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
